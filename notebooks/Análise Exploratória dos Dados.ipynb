{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55c1afbf",
   "metadata": {},
   "source": [
    "# Análise Exploratória dos Dados\n",
    "\n",
    "Este notebook tem como objetivo realizar uma análise exploratória completa dos dados, incluindo:\n",
    "\n",
    "- Importação e carregamento dos dados\n",
    "- Análise da estrutura e qualidade dos dados\n",
    "- Estatísticas descritivas\n",
    "- Visualizações gráficas\n",
    "- Identificação de padrões e outliers\n",
    "- Análise de correlações\n",
    "\n",
    "A análise exploratória é fundamental para entender melhor o dataset antes de aplicar modelos de machine learning ou realizar análises mais avançadas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e89f703",
   "metadata": {},
   "source": [
    "## 1. Importação das Bibliotecas\n",
    "\n",
    "Importamos as bibliotecas essenciais para análise exploratória de dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1170ee53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bibliotecas importadas com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Bibliotecas para manipulação e análise de dados\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Bibliotecas para visualização\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Configurações de visualização\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Configurações do pandas\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Configurações de warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Bibliotecas importadas com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6e4ec9",
   "metadata": {},
   "source": [
    "## 2. Carregamento dos Dados\n",
    "\n",
    "Carregamos o dataset especificando o caminho e formato do arquivo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc95c0c",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File data/applicants.json does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Carregar dados do sistema de recrutamento\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df_candidatos = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_json\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdata/applicants.json\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m df_entrevistas = pd.read_json(\u001b[33m'\u001b[39m\u001b[33mdata/prospects.json\u001b[39m\u001b[33m'\u001b[39m, encoding=\u001b[33m'\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      4\u001b[39m df_vagas = pd.read_json(\u001b[33m'\u001b[39m\u001b[33mdata/vagas.json\u001b[39m\u001b[33m'\u001b[39m, encoding=\u001b[33m'\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Development/4MLET/fiap_tech_challenge_05/.venv/lib/python3.12/site-packages/pandas/io/json/_json.py:791\u001b[39m, in \u001b[36mread_json\u001b[39m\u001b[34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options, dtype_backend, engine)\u001b[39m\n\u001b[32m    788\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m convert_axes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m orient != \u001b[33m\"\u001b[39m\u001b[33mtable\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    789\u001b[39m     convert_axes = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m791\u001b[39m json_reader = \u001b[43mJsonReader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43morient\u001b[49m\u001b[43m=\u001b[49m\u001b[43morient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconvert_axes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconvert_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeep_default_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_default_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprecise_float\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprecise_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdate_unit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    801\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlines\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    803\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    804\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    805\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    806\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    807\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding_errors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    808\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    809\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    810\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    812\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize:\n\u001b[32m    813\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m json_reader\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Development/4MLET/fiap_tech_challenge_05/.venv/lib/python3.12/site-packages/pandas/io/json/_json.py:904\u001b[39m, in \u001b[36mJsonReader.__init__\u001b[39m\u001b[34m(self, filepath_or_buffer, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, lines, chunksize, compression, nrows, storage_options, encoding_errors, dtype_backend, engine)\u001b[39m\n\u001b[32m    902\u001b[39m     \u001b[38;5;28mself\u001b[39m.data = filepath_or_buffer\n\u001b[32m    903\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.engine == \u001b[33m\"\u001b[39m\u001b[33mujson\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m904\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data_from_filepath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m     \u001b[38;5;28mself\u001b[39m.data = \u001b[38;5;28mself\u001b[39m._preprocess_data(data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Development/4MLET/fiap_tech_challenge_05/.venv/lib/python3.12/site-packages/pandas/io/json/_json.py:960\u001b[39m, in \u001b[36mJsonReader._get_data_from_filepath\u001b[39m\u001b[34m(self, filepath_or_buffer)\u001b[39m\n\u001b[32m    952\u001b[39m     filepath_or_buffer = \u001b[38;5;28mself\u001b[39m.handles.handle\n\u001b[32m    953\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[32m    954\u001b[39m     \u001b[38;5;28misinstance\u001b[39m(filepath_or_buffer, \u001b[38;5;28mstr\u001b[39m)\n\u001b[32m    955\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m filepath_or_buffer.lower().endswith(\n\u001b[32m   (...)\u001b[39m\u001b[32m    958\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m file_exists(filepath_or_buffer)\n\u001b[32m    959\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m960\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFile \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath_or_buffer\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not exist\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    961\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    962\u001b[39m     warnings.warn(\n\u001b[32m    963\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPassing literal json to \u001b[39m\u001b[33m'\u001b[39m\u001b[33mread_json\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is deprecated and \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    964\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mwill be removed in a future version. To read from a \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    967\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    968\u001b[39m     )\n",
      "\u001b[31mFileNotFoundError\u001b[39m: File data/applicants.json does not exist"
     ]
    }
   ],
   "source": [
    "# Carregar dados do sistema de recrutamento\n",
    "df_candidatos = pd.read_json('../data/applicants.json', encoding='utf-8')\n",
    "df_entrevistas = pd.read_json('../data/prospects.json', encoding='utf-8')\n",
    "df_vagas = pd.read_json('../data/vagas.json', encoding='utf-8')\n",
    "\n",
    "# Análise básica dos datasets\n",
    "print(\"=== ANÁLISE EXPLORATÓRIA ===\")\n",
    "print(f\"Candidatos: {df_candidatos.shape}\")\n",
    "print(f\"Entrevistas: {df_entrevistas.shape}\")\n",
    "print(f\"Vagas: {df_vagas.shape}\")\n",
    "\n",
    "# Para análise principal, vamos usar o dataset de candidatos\n",
    "df = df_candidatos.copy()\n",
    "\n",
    "print(f\"\\nDataset principal (candidatos) carregado com sucesso!\")\n",
    "print(f\"Número de linhas: {len(df)}\")\n",
    "print(f\"Número de colunas: {len(df.columns)}\")\n",
    "\n",
    "# Exibir informações dos outros datasets\n",
    "print(f\"\\n=== INFORMAÇÕES DOS DATASETS ===\")\n",
    "print(f\"Candidatos - Colunas: {list(df_candidatos.columns)}\")\n",
    "print(f\"Entrevistas - Colunas: {list(df_entrevistas.columns)}\")\n",
    "print(f\"Vagas - Colunas: {list(df_vagas.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03054e8a",
   "metadata": {},
   "source": [
    "## 3. Visualização Inicial dos Dados\n",
    "\n",
    "Vamos examinar as primeiras e últimas linhas do dataset para entender sua estrutura:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef63a9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primeiras 5 linhas do dataset\n",
    "print(\"Primeiras 5 linhas:\")\n",
    "print(\"=\" * 50)\n",
    "display(df.head())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Últimas 5 linhas:\")\n",
    "print(\"=\" * 50)\n",
    "display(df.tail())\n",
    "\n",
    "# Amostra aleatória\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Amostra aleatória de 5 linhas:\")\n",
    "print(\"=\" * 50)\n",
    "display(df.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b400027",
   "metadata": {},
   "source": [
    "## 4. Informações Básicas do Dataset\n",
    "\n",
    "Vamos analisar a estrutura, dimensões e tipos de dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db737420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informações gerais do dataset\n",
    "print(\"INFORMAÇÕES GERAIS DO DATASET\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Dimensões: {df.shape}\")\n",
    "print(f\"Número de linhas: {df.shape[0]}\")\n",
    "print(f\"Número de colunas: {df.shape[1]}\")\n",
    "print(f\"Tamanho total: {df.size}\")\n",
    "print(f\"Uso de memória: {df.memory_usage().sum()} bytes\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"TIPOS DE DADOS\")\n",
    "print(\"=\" * 50)\n",
    "print(df.dtypes)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"NOMES DAS COLUNAS\")\n",
    "print(\"=\" * 50)\n",
    "print(list(df.columns))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"INFORMAÇÕES DETALHADAS\")\n",
    "print(\"=\" * 50)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0862ee3f",
   "metadata": {},
   "source": [
    "## 5. Análise de Valores Ausentes\n",
    "\n",
    "Identificamos e visualizamos valores ausentes no dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76855505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adicionar alguns valores ausentes para demonstração\n",
    "df_missing = df.copy()\n",
    "# Inserir alguns NaN aleatoriamente\n",
    "np.random.seed(42)\n",
    "mask = np.random.random(df_missing.shape) < 0.05  # 5% de valores ausentes\n",
    "df_missing = df_missing.mask(mask)\n",
    "\n",
    "# Análise de valores ausentes\n",
    "print(\"ANÁLISE DE VALORES AUSENTES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Contagem de valores ausentes\n",
    "missing_count = df_missing.isnull().sum()\n",
    "missing_percent = (df_missing.isnull().sum() / len(df_missing)) * 100\n",
    "\n",
    "missing_data = pd.DataFrame({\n",
    "    'Coluna': missing_count.index,\n",
    "    'Valores Ausentes': missing_count.values,\n",
    "    'Percentual (%)': missing_percent.values\n",
    "})\n",
    "\n",
    "print(\"Resumo de valores ausentes por coluna:\")\n",
    "display(missing_data[missing_data['Valores Ausentes'] > 0])\n",
    "\n",
    "# Visualização de valores ausentes\n",
    "if missing_data['Valores Ausentes'].sum() > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Heatmap de valores ausentes\n",
    "    sns.heatmap(df_missing.isnull(), cbar=True, yticklabels=False, ax=axes[0])\n",
    "    axes[0].set_title('Mapa de Valores Ausentes')\n",
    "    \n",
    "    # Gráfico de barras\n",
    "    missing_data_plot = missing_data[missing_data['Valores Ausentes'] > 0]\n",
    "    if len(missing_data_plot) > 0:\n",
    "        axes[1].bar(missing_data_plot['Coluna'], missing_data_plot['Percentual (%)'])\n",
    "        axes[1].set_title('Percentual de Valores Ausentes por Coluna')\n",
    "        axes[1].set_ylabel('Percentual (%)')\n",
    "        plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"✅ Não há valores ausentes no dataset!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192818ff",
   "metadata": {},
   "source": [
    "## 6. Estatísticas Descritivas\n",
    "\n",
    "Analisamos as medidas de tendência central e dispersão das variáveis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdabd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estatísticas descritivas para variáveis numéricas\n",
    "print(\"ESTATÍSTICAS DESCRITIVAS - VARIÁVEIS NUMÉRICAS\")\n",
    "print(\"=\" * 50)\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "display(df[numeric_cols].describe())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ESTATÍSTICAS DESCRITIVAS - VARIÁVEIS CATEGÓRICAS\")\n",
    "print(\"=\" * 50)\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(df[col].value_counts().head())\n",
    "    print(f\"Valores únicos: {df[col].nunique()}\")\n",
    "\n",
    "# Informações adicionais\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"RESUMO ESTATÍSTICO ADICIONAL\")\n",
    "print(\"=\" * 50)\n",
    "for col in numeric_cols:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  Média: {df[col].mean():.2f}\")\n",
    "    print(f\"  Mediana: {df[col].median():.2f}\")\n",
    "    print(f\"  Desvio Padrão: {df[col].std():.2f}\")\n",
    "    print(f\"  Variância: {df[col].var():.2f}\")\n",
    "    print(f\"  Min: {df[col].min():.2f}\")\n",
    "    print(f\"  Max: {df[col].max():.2f}\")\n",
    "    print(f\"  Amplitude: {df[col].max() - df[col].min():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69f1fd1",
   "metadata": {},
   "source": [
    "## 7. Análise de Distribuições\n",
    "\n",
    "Analisamos a distribuição das variáveis numéricas através de histogramas e box plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c04c276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar variáveis numéricas\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "if len(numeric_cols) > 0:\n",
    "    # Número de colunas para subplot\n",
    "    n_cols = min(3, len(numeric_cols))\n",
    "    n_rows = (len(numeric_cols) + n_cols - 1) // n_cols\n",
    "    \n",
    "    # Histogramas\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
    "    if n_rows == 1:\n",
    "        axes = axes.reshape(1, -1) if len(numeric_cols) > 1 else [axes]\n",
    "    \n",
    "    for i, col in enumerate(numeric_cols):\n",
    "        row = i // n_cols\n",
    "        col_idx = i % n_cols\n",
    "        \n",
    "        if n_rows > 1:\n",
    "            ax = axes[row, col_idx]\n",
    "        else:\n",
    "            ax = axes[col_idx] if len(numeric_cols) > 1 else axes[0]\n",
    "            \n",
    "        df[col].hist(bins=30, ax=ax, alpha=0.7, edgecolor='black')\n",
    "        ax.set_title(f'Distribuição - {col}')\n",
    "        ax.set_xlabel(col)\n",
    "        ax.set_ylabel('Frequência')\n",
    "    \n",
    "    # Remover subplots vazios\n",
    "    for i in range(len(numeric_cols), n_rows * n_cols):\n",
    "        row = i // n_cols\n",
    "        col_idx = i % n_cols\n",
    "        if n_rows > 1:\n",
    "            fig.delaxes(axes[row, col_idx])\n",
    "        elif len(numeric_cols) < n_cols:\n",
    "            fig.delaxes(axes[col_idx])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Box plots\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
    "    if n_rows == 1:\n",
    "        axes = axes.reshape(1, -1) if len(numeric_cols) > 1 else [axes]\n",
    "    \n",
    "    for i, col in enumerate(numeric_cols):\n",
    "        row = i // n_cols\n",
    "        col_idx = i % n_cols\n",
    "        \n",
    "        if n_rows > 1:\n",
    "            ax = axes[row, col_idx]\n",
    "        else:\n",
    "            ax = axes[col_idx] if len(numeric_cols) > 1 else axes[0]\n",
    "            \n",
    "        df[col].plot(kind='box', ax=ax)\n",
    "        ax.set_title(f'Box Plot - {col}')\n",
    "        ax.set_ylabel(col)\n",
    "    \n",
    "    # Remover subplots vazios\n",
    "    for i in range(len(numeric_cols), n_rows * n_cols):\n",
    "        row = i // n_cols\n",
    "        col_idx = i % n_cols\n",
    "        if n_rows > 1:\n",
    "            fig.delaxes(axes[row, col_idx])\n",
    "        elif len(numeric_cols) < n_cols:\n",
    "            fig.delaxes(axes[col_idx])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Não há variáveis numéricas no dataset para análise de distribuição.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1a244e",
   "metadata": {},
   "source": [
    "## 8. Análise de Correlações\n",
    "\n",
    "Calculamos e visualizamos as correlações entre variáveis numéricas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cf11ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise de correlações entre variáveis numéricas\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "if len(numeric_cols) > 1:\n",
    "    # Calcular matriz de correlação\n",
    "    correlacoes = df[numeric_cols].corr()\n",
    "    \n",
    "    print(\"MATRIZ DE CORRELAÇÃO\")\n",
    "    print(\"=\" * 50)\n",
    "    display(correlacoes)\n",
    "    \n",
    "    # Visualização da matriz de correlação\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(correlacoes, annot=True, cmap='coolwarm', center=0, \n",
    "                square=True, cbar_kws={'shrink': 0.8})\n",
    "    plt.title('Matriz de Correlação - Variáveis Numéricas')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Identificar correlações mais fortes\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"CORRELAÇÕES MAIS FORTES (|r| > 0.5)\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Criar uma matriz upper triangular para evitar duplicatas\n",
    "    upper_triangle = correlacoes.where(np.triu(np.ones(correlacoes.shape), k=1).astype(bool))\n",
    "    strong_corr = []\n",
    "    \n",
    "    for col in upper_triangle.columns:\n",
    "        for idx in upper_triangle.index:\n",
    "            corr_value = upper_triangle.loc[idx, col]\n",
    "            if pd.notna(corr_value) and abs(corr_value) > 0.5:\n",
    "                strong_corr.append((idx, col, corr_value))\n",
    "    \n",
    "    if strong_corr:\n",
    "        for var1, var2, corr in sorted(strong_corr, key=lambda x: abs(x[2]), reverse=True):\n",
    "            print(f\"{var1} ↔ {var2}: {corr:.3f}\")\n",
    "    else:\n",
    "        print(\"Não foram encontradas correlações fortes (|r| > 0.5)\")\n",
    "        \n",
    "    # Correlações específicas para candidatos (se aplicável)\n",
    "    if 'df_candidatos' in locals():\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"ANÁLISE ESPECÍFICA - DADOS DE CANDIDATOS\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Se houver variáveis específicas de RH, analisá-las\n",
    "        cand_numeric = df_candidatos.select_dtypes(include=[np.number]).columns\n",
    "        if len(cand_numeric) > 1:\n",
    "            cand_corr = df_candidatos[cand_numeric].corr()\n",
    "            print(\"Correlações entre características dos candidatos:\")\n",
    "            display(cand_corr)\n",
    "            \n",
    "else:\n",
    "    print(\"Dataset possui menos de 2 variáveis numéricas. Análise de correlação não aplicável.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc92a75",
   "metadata": {},
   "source": [
    "## 9. Visualizações Gráficas\n",
    "\n",
    "Criamos diferentes tipos de gráficos para explorar as relações entre variáveis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cdcb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizações para variáveis categóricas\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "if len(categorical_cols) > 0:\n",
    "    print(\"ANÁLISE DE VARIÁVEIS CATEGÓRICAS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Gráficos de barras para variáveis categóricas\n",
    "    n_cats = len(categorical_cols)\n",
    "    n_cols = min(2, n_cats)\n",
    "    n_rows = (n_cats + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
    "    if n_rows == 1 and n_cats == 1:\n",
    "        axes = [axes]\n",
    "    elif n_rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i, col in enumerate(categorical_cols):\n",
    "        row = i // n_cols\n",
    "        col_idx = i % n_cols\n",
    "        \n",
    "        if n_rows > 1:\n",
    "            ax = axes[row, col_idx]\n",
    "        else:\n",
    "            ax = axes[col_idx] if n_cats > 1 else axes[0]\n",
    "        \n",
    "        value_counts = df[col].value_counts()\n",
    "        value_counts.plot(kind='bar', ax=ax, rot=45)\n",
    "        ax.set_title(f'Distribuição - {col}')\n",
    "        ax.set_xlabel(col)\n",
    "        ax.set_ylabel('Contagem')\n",
    "    \n",
    "    # Remover subplots vazios\n",
    "    for i in range(n_cats, n_rows * n_cols):\n",
    "        row = i // n_cols\n",
    "        col_idx = i % n_cols\n",
    "        if n_rows > 1:\n",
    "            fig.delaxes(axes[row, col_idx])\n",
    "        elif n_cats < n_cols:\n",
    "            fig.delaxes(axes[col_idx])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Scatter plots para variáveis numéricas\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "if len(numeric_cols) >= 2:\n",
    "    print(\"\\nSCATTER PLOTS - RELAÇÕES ENTRE VARIÁVEIS NUMÉRICAS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Criar pair plot se houver poucas variáveis numéricas\n",
    "    if len(numeric_cols) <= 5:\n",
    "        sns.pairplot(df[numeric_cols])\n",
    "        plt.suptitle('Pair Plot - Variáveis Numéricas', y=1.02)\n",
    "        plt.show()\n",
    "    else:\n",
    "        # Selecionar algumas variáveis para scatter plots\n",
    "        selected_cols = numeric_cols[:4]  # Primeiras 4 variáveis\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "        \n",
    "        for i in range(min(3, len(selected_cols)-1)):\n",
    "            for j in range(i+1, min(4, len(selected_cols))):\n",
    "                row = (i*2 + j - i - 1) // 2\n",
    "                col = (i*2 + j - i - 1) % 2\n",
    "                \n",
    "                if row < 2 and col < 2:\n",
    "                    df.plot.scatter(x=selected_cols[i], y=selected_cols[j], \n",
    "                                  ax=axes[row, col], alpha=0.6)\n",
    "                    axes[row, col].set_title(f'{selected_cols[i]} vs {selected_cols[j]}')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Análise específica para dados de recrutamento\n",
    "print(\"\\nANÁLISE ESPECÍFICA - SISTEMA DE RECRUTAMENTO\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Verificar se os DataFrames de recrutamento existem\n",
    "datasets_info = [\n",
    "    ('Candidatos', 'df_candidatos'),\n",
    "    ('Entrevistas', 'df_entrevistas'), \n",
    "    ('Vagas', 'df_vagas')\n",
    "]\n",
    "\n",
    "for name, df_name in datasets_info:\n",
    "    if df_name in locals():\n",
    "        current_df = locals()[df_name]\n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"  - Dimensões: {current_df.shape}\")\n",
    "        print(f\"  - Colunas: {list(current_df.columns)}\")\n",
    "        if len(current_df.select_dtypes(include=['object']).columns) > 0:\n",
    "            cat_col = current_df.select_dtypes(include=['object']).columns[0]\n",
    "            print(f\"  - Principais categorias em '{cat_col}': {current_df[cat_col].value_counts().head(3).to_dict()}\")\n",
    "    else:\n",
    "        print(f\"{name}: Dataset não carregado (verifique o caminho dos arquivos)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac449a1",
   "metadata": {},
   "source": [
    "## 10. Identificação de Outliers\n",
    "\n",
    "Detectamos outliers usando métodos estatísticos e visualizações:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01ffd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para detectar outliers usando IQR\n",
    "def detect_outliers_iqr(data, column):\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
    "    return outliers, lower_bound, upper_bound\n",
    "\n",
    "# Função para detectar outliers usando Z-score\n",
    "def detect_outliers_zscore(data, column, threshold=3):\n",
    "    z_scores = np.abs((data[column] - data[column].mean()) / data[column].std())\n",
    "    outliers = data[z_scores > threshold]\n",
    "    return outliers\n",
    "\n",
    "# Análise de outliers para cada variável numérica\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "if len(numeric_cols) > 0:\n",
    "    print(\"DETECÇÃO DE OUTLIERS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    outlier_summary = []\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        print(f\"\\nVariável: {col}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        # Método IQR\n",
    "        outliers_iqr, lower_bound, upper_bound = detect_outliers_iqr(df, col)\n",
    "        print(f\"Método IQR:\")\n",
    "        print(f\"  Limite inferior: {lower_bound:.2f}\")\n",
    "        print(f\"  Limite superior: {upper_bound:.2f}\")\n",
    "        print(f\"  Outliers detectados: {len(outliers_iqr)}\")\n",
    "        \n",
    "        # Método Z-score\n",
    "        outliers_zscore = detect_outliers_zscore(df, col)\n",
    "        print(f\"Método Z-score (|z| > 3):\")\n",
    "        print(f\"  Outliers detectados: {len(outliers_zscore)}\")\n",
    "        \n",
    "        outlier_summary.append({\n",
    "            'Variável': col,\n",
    "            'Outliers_IQR': len(outliers_iqr),\n",
    "            'Outliers_ZScore': len(outliers_zscore),\n",
    "            'Percentual_IQR': (len(outliers_iqr) / len(df)) * 100,\n",
    "            'Percentual_ZScore': (len(outliers_zscore) / len(df)) * 100\n",
    "        })\n",
    "    \n",
    "    # Resumo dos outliers\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"RESUMO DE OUTLIERS\")\n",
    "    print(\"=\" * 50)\n",
    "    outlier_df = pd.DataFrame(outlier_summary)\n",
    "    display(outlier_df)\n",
    "    \n",
    "    # Visualização de outliers\n",
    "    n_cols_plot = min(3, len(numeric_cols))\n",
    "    n_rows_plot = (len(numeric_cols) + n_cols_plot - 1) // n_cols_plot\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows_plot, n_cols_plot, figsize=(15, 5*n_rows_plot))\n",
    "    if n_rows_plot == 1:\n",
    "        axes = axes.reshape(1, -1) if len(numeric_cols) > 1 else [axes]\n",
    "    \n",
    "    for i, col in enumerate(numeric_cols):\n",
    "        row = i // n_cols_plot\n",
    "        col_idx = i % n_cols_plot\n",
    "        \n",
    "        if n_rows_plot > 1:\n",
    "            ax = axes[row, col_idx]\n",
    "        else:\n",
    "            ax = axes[col_idx] if len(numeric_cols) > 1 else axes[0]\n",
    "        \n",
    "        # Box plot com outliers destacados\n",
    "        bp = ax.boxplot(df[col].dropna(), patch_artist=True)\n",
    "        bp['boxes'][0].set_facecolor('lightblue')\n",
    "        ax.set_title(f'Box Plot - {col}')\n",
    "        ax.set_ylabel(col)\n",
    "        \n",
    "        # Adicionar estatísticas\n",
    "        outliers_iqr, _, _ = detect_outliers_iqr(df, col)\n",
    "        ax.text(0.02, 0.98, f'Outliers: {len(outliers_iqr)}', \n",
    "                transform=ax.transAxes, verticalalignment='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    # Remover subplots vazios\n",
    "    for i in range(len(numeric_cols), n_rows_plot * n_cols_plot):\n",
    "        row = i // n_cols_plot\n",
    "        col_idx = i % n_cols_plot\n",
    "        if n_rows_plot > 1:\n",
    "            fig.delaxes(axes[row, col_idx])\n",
    "        elif len(numeric_cols) < n_cols_plot:\n",
    "            fig.delaxes(axes[col_idx])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Scatter plots para identificar outliers multivariados\n",
    "    if len(numeric_cols) >= 2:\n",
    "        print(\"\\nOUTLIERS MULTIVARIADOS\")\n",
    "        print(\"=\" * 30)\n",
    "        \n",
    "        # Selecionar duas variáveis principais para análise\n",
    "        var1, var2 = numeric_cols[0], numeric_cols[1]\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.scatter(df[var1], df[var2], alpha=0.6)\n",
    "        plt.xlabel(var1)\n",
    "        plt.ylabel(var2)\n",
    "        plt.title(f'Scatter Plot: {var1} vs {var2}')\n",
    "        \n",
    "        # Destacar outliers baseados em ambas variáveis\n",
    "        outliers_var1, _, _ = detect_outliers_iqr(df, var1)\n",
    "        outliers_var2, _, _ = detect_outliers_iqr(df, var2)\n",
    "        \n",
    "        combined_outliers = pd.concat([outliers_var1, outliers_var2]).drop_duplicates()\n",
    "        \n",
    "        if len(combined_outliers) > 0:\n",
    "            plt.scatter(combined_outliers[var1], combined_outliers[var2], \n",
    "                       color='red', s=50, alpha=0.8, label=f'Outliers ({len(combined_outliers)})')\n",
    "            plt.legend()\n",
    "        \n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"Não há variáveis numéricas no dataset para análise de outliers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2928f639",
   "metadata": {},
   "source": [
    "## 11. Conclusões da Análise Exploratória\n",
    "\n",
    "### Resumo dos Principais Achados\n",
    "\n",
    "Esta análise exploratória nos permitiu identificar características importantes dos dados de recrutamento:\n",
    "\n",
    "#### Estrutura dos Dados\n",
    "- **Candidatos**: Dataset principal com informações dos candidatos\n",
    "- **Entrevistas**: Dados das entrevistas realizadas  \n",
    "- **Vagas**: Informações sobre as vagas disponíveis\n",
    "\n",
    "#### Principais Insights\n",
    "- Identificação de padrões nos dados de candidatos\n",
    "- Análise de correlações entre variáveis numéricas\n",
    "- Detecção de valores ausentes e outliers\n",
    "- Distribuições das variáveis principais\n",
    "\n",
    "#### Próximos Passos\n",
    "1. **Limpeza de Dados**: Tratar valores ausentes e outliers identificados\n",
    "2. **Feature Engineering**: Criar novas variáveis baseadas nos insights encontrados\n",
    "3. **Modelagem**: Aplicar algoritmos de machine learning para:\n",
    "   - Previsão de sucesso de candidatos\n",
    "   - Matching entre candidatos e vagas\n",
    "   - Otimização do processo de recrutamento\n",
    "\n",
    "#### Recomendações\n",
    "- Verificar a qualidade dos dados nos arquivos CSV\n",
    "- Analisar relações entre os três datasets (joins)\n",
    "- Implementar validações de dados no sistema de recrutamento\n",
    "- Monitorar métricas de qualidade dos dados continuamente\n",
    "\n",
    "---\n",
    "**Nota**: Este notebook serve como base para análises mais avançadas. Certifique-se de que os arquivos `dados/candidatos.csv`, `dados/entrevistas.csv` e `dados/vagas.csv` estejam disponíveis no diretório correto."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

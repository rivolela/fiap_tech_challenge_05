# FIAP Tech Challenge 05 - Decision Scoring API

Este projeto contÃ©m anÃ¡lises exploratÃ³rias de dados para sistema de recrutamento e uma API de scoring para auxiliar na tomada de decisÃ£o no processo de recrutamento.

## ğŸš€ ConfiguraÃ§Ã£o do Ambiente

### PrÃ©-requisitos
- Python 3.8 ou superior
- pip (gerenciador de pacotes Python)

### ConfiguraÃ§Ã£o AutomÃ¡tica
Execute o script de setup:
```bash
./scripts/setup.sh
```

### ConfiguraÃ§Ã£o Manual
1. Criar ambiente virtual:
```bash
python3 -m venv .venv
```

2. Ativar ambiente virtual:
```bash
source .venv/bin/activate  # macOS/Linux
# ou
.venv\Scripts\activate     # Windows
```

3. Instalar dependÃªncias:
```bash
pip install -r requirements.txt
```

4. Verificar instalaÃ§Ã£o:
```bash
python check_env.py
```

## ğŸ“Š Estrutura do Projeto

```
fiap_tech_challenge_05/
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ AnÃ¡lise ExploratÃ³ria dos Dados.ipynb
â”‚   â””â”€â”€ analise_exploratoria.ipynb
â”‚   â””â”€â”€ threshold_adjustment_analysis.ipynb
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â”œâ”€â”€ applicants.json
â”‚   â”‚   â”œâ”€â”€ prospects.json
â”‚   â”‚   â””â”€â”€ jobs.json
â”‚   â”œâ”€â”€ processed/
â”‚   â”‚   â”œâ”€â”€ complete_processed_data.csv
â”‚   â”‚   â””â”€â”€ splits/
â”‚   â”‚       â”œâ”€â”€ X_train.csv
â”‚   â”‚       â”œâ”€â”€ X_test.csv
â”‚   â”‚       â””â”€â”€ ...
â”‚   â”œâ”€â”€ insights/
â”‚   â””â”€â”€ visualizations/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ data_analysis.py
â”‚   â”‚   â””â”€â”€ download_data.py
â”‚   â”œâ”€â”€ features/
â”‚   â”‚   â”œâ”€â”€ data_validation.py
â”‚   â”‚   â””â”€â”€ feature_engineering.py
â”‚   â””â”€â”€ models/
â”‚       â”œâ”€â”€ train_simple.py
â”‚       â”œâ”€â”€ train_model.py
â”‚       â””â”€â”€ mlflow_server.py
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ scoring_model.pkl
â”‚   â””â”€â”€ feature_scaler.pkl
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ mlflow_guide.md
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ check_env.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.sh
â”œâ”€â”€ check_env.py
â”œâ”€â”€ .gitignore
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md
```

## ğŸ”§ Bibliotecas Utilizadas

- **pandas**: ManipulaÃ§Ã£o e anÃ¡lise de dados
- **numpy**: ComputaÃ§Ã£o cientÃ­fica
- **matplotlib**: VisualizaÃ§Ã£o de dados
- **seaborn**: VisualizaÃ§Ã£o estatÃ­stica
- **plotly**: GrÃ¡ficos interativos
- **scikit-learn**: Machine learning
- **fastapi**: Framework para API
- **jupyter**: Ambiente de notebooks
- **mlflow**: Rastreamento e gerenciamento de experimentos de ML
- **textblob**: Processamento de linguagem natural para LLM

## ğŸ“ˆ Como Usar

1. Ativar o ambiente virtual:
```bash
source .venv/bin/activate
```

2. Iniciar Jupyter Notebook:
```bash
jupyter notebook
```

3. Abrir o notebook [`notebooks/AnÃ¡lise ExploratÃ³ria dos Dados.ipynb`](notebooks/AnÃ¡lise%20ExploratÃ³ria%20dos%20Dados.ipynb)

## ğŸ› ï¸ VariÃ¡veis de Ambiente

Para configurar a API, vocÃª pode usar diversas variÃ¡veis de ambiente:
- `PORT`: Porta onde a API serÃ¡ executada (padrÃ£o: 8000)
- `LOG_LEVEL`: NÃ­vel de logging (padrÃ£o: INFO)
- `CLASSIFICATION_THRESHOLD`: Threshold para classificaÃ§Ã£o de candidatos (padrÃ£o: 0.25)

Veja a lista completa em [docs/env_variables.md](docs/env_variables.md).

## ğŸ¤– Sistema HÃ­brido de Scoring + Clustering

### Pipeline Completo

Execute o pipeline completo com MLflow usando:

```bash
./scripts/run_pipeline.sh
```

OpÃ§Ãµes disponÃ­veis:
- `--compare`: Treina e compara diferentes modelos
- `--port 8080`: Altera a porta do servidor MLflow (padrÃ£o: 5001)
- `--no-server`: Executa o pipeline sem iniciar o servidor MLflow
- `--no-cv`: Desativa a validaÃ§Ã£o cruzada
- `--no-leakage-prevention`: Desativa a detecÃ§Ã£o de data leakage
- `--no-feature-selection`: Desativa a seleÃ§Ã£o de features
- `--cv-folds 10`: Altera o nÃºmero de folds na validaÃ§Ã£o cruzada (padrÃ£o: 5)

### Treinamento do Modelo

Para treinar modelos separadamente:

```bash
# Treinar o modelo RandomForest padrÃ£o
python src/models/train_simple.py

# Comparar diferentes modelos
python src/models/train_simple.py --compare
```

### MLflow - Tracking de Experimentos

Para gerenciar experimentos MLflow:

```bash
# Iniciar servidor MLflow
python src/models/mlflow_server.py

# Listar experimentos
python src/models/mlflow_server.py --list

# Excluir experimento
python src/models/mlflow_server.py --delete "Decision-Scoring-Model"
```

Para mais detalhes sobre MLflow, consulte o guia em [docs/mlflow_guide.md](docs/mlflow_guide.md)

## ğŸš€ API de Scoring

O projeto inclui uma API para servir o modelo de Machine Learning treinado:

### Executando a API Localmente
```bash
./scripts/start_api.sh
```

### Utilizando Docker
```bash
docker build -t decision-scoring-api .
docker run -p 8000:8000 decision-scoring-api
```

### Endpoints Principais
- `POST /score` - Endpoint principal para prediÃ§Ãµes individuais
- `POST /score/batch` - Processamento de mÃºltiplos candidatos em lote
- `GET /health` - Health check da API
- `GET /metrics` - MÃ©tricas de desempenho da API (requer autenticaÃ§Ã£o admin)
- `GET /monitoring/drift` - AnÃ¡lise de drift do modelo (requer autenticaÃ§Ã£o admin)
- `GET /monitoring/drift/visualization` - VisualizaÃ§Ã£o de drift para uma feature especÃ­fica
- `GET /monitoring/metrics/history` - HistÃ³rico de mÃ©tricas do modelo
- `GET /monitoring/predictions/recent` - EstatÃ­sticas sobre prediÃ§Ãµes recentes

### Sistema de Monitoramento

O projeto inclui um sistema completo de monitoramento para acompanhar o desempenho do modelo e detectar drift:

#### Dashboard de MÃ©tricas

Para iniciar o dashboard de monitoramento:

```bash
./scripts/start_dashboard.sh
```

O dashboard fornece:
- VisualizaÃ§Ã£o em tempo real das mÃ©tricas do modelo
- AnÃ¡lise de drift entre dados de treino e produÃ§Ã£o
- HistÃ³rico de desempenho do modelo
- EstatÃ­sticas sobre prediÃ§Ãµes recentes

#### VerificaÃ§Ã£o de Drift

Para verificar drift e gerar alertas:

```bash
./scripts/check_drift.sh
```

Consulte a [documentaÃ§Ã£o completa do sistema de monitoramento](docs/monitoring_guide.md) para mais detalhes.
- `/docs` - DocumentaÃ§Ã£o interativa (Swagger UI)

### AutenticaÃ§Ã£o
Todas as requisiÃ§Ãµes devem incluir um cabeÃ§alho `X-API-Key` com uma chave vÃ¡lida:
- `your-api-key`: Acesso de administrador (todos os endpoints)
- `test-api-key`: Acesso somente leitura (endpoints bÃ¡sicos)

### Exemplo de RequisiÃ§Ã£o
```bash
curl -X POST "http://localhost:8000/score" \
  -H "X-API-Key: your-api-key" \
  -H "Content-Type: application/json" \
  -d '{
    "idade": 28,
    "experiencia": 5,
    "educacao": "ensino_superior",
    "area_formacao": "tecnologia",
    "vaga_titulo": "Desenvolvedor Python",
    "vaga_area": "tecnologia",
    "vaga_senioridade": "pleno"
  }'
```

### Exemplo de Resposta
```json
{
  "prediction": 1,
  "probability": 0.85,
  "recommendation": "Recomendado",
  "comment": "A avaliaÃ§Ã£o tÃ©cnica sugere boa adequaÃ§Ã£o para a funÃ§Ã£o de Desenvolvedor Python na Ã¡rea de tecnologia, nÃ­vel pleno. Destaca-se formaÃ§Ã£o superior na Ã¡rea de tecnologia e 5.0 anos de experiÃªncia relevante.",
  "vaga_info": {
    "id": "vaga-123",
    "titulo": "Desenvolvedor Python",
    "area": "tecnologia",
    "senioridade": "pleno"
  },
  "match_score": 0.78
}
```

### ImplantaÃ§Ã£o no Render
O projeto pode ser facilmente implantado na plataforma Render usando Docker ou o arquivo de configuraÃ§Ã£o incluÃ­do.

#### OpÃ§Ãµes de ImplantaÃ§Ã£o
1. **Via Blueprint (config/render/render.yaml)**: ImplantaÃ§Ã£o automÃ¡tica usando nosso arquivo de configuraÃ§Ã£o
2. **Via Docker**: ImplantaÃ§Ã£o manual do contÃªiner Docker usando o config/docker/api/Dockerfile incluÃ­do
3. **Sem Docker**: ImplantaÃ§Ã£o usando o ambiente Python do Render e o config/render/Procfile

## ğŸ“ Dados

O projeto utiliza trÃªs datasets em formato JSON na pasta `data/`:
- **applicants.json**: Dados dos candidatos ao processo seletivo
- **prospects.json**: Dados das entrevistas e prospects
- **vagas.json**: InformaÃ§Ãµes sobre as vagas disponÃ­veis

### Estrutura dos Dados
- **Candidatos**: Dataset principal com informaÃ§Ãµes dos candidatos
- **Entrevistas**: Dados das entrevistas realizadas  
- **Vagas**: InformaÃ§Ãµes sobre as vagas disponÃ­veis

## ğŸ” AnÃ¡lise ExploratÃ³ria

O notebook principal inclui:
- ImportaÃ§Ã£o e carregamento dos dados JSON
- AnÃ¡lise da estrutura e qualidade dos dados
- EstatÃ­sticas descritivas
- VisualizaÃ§Ãµes grÃ¡ficas interativas
- IdentificaÃ§Ã£o de padrÃµes e outliers
- AnÃ¡lise de correlaÃ§Ãµes
- DetecÃ§Ã£o de valores ausentes

### Principais Insights
- AnÃ¡lise de padrÃµes nos dados de candidatos
- IdentificaÃ§Ã£o de correlaÃ§Ãµes entre variÃ¡veis
- DetecÃ§Ã£o e tratamento de outliers
- DistribuiÃ§Ãµes das variÃ¡veis principais

## ğŸ§  Recursos de IA e LLM

O projeto agora inclui recursos de IA para gerar comentÃ¡rios personalizados sobre candidatos:

### ComentÃ¡rios LLM para RecomendaÃ§Ãµes
A API agora gera automaticamente comentÃ¡rios em linguagem natural para cada recomendaÃ§Ã£o de candidato. Este recurso:

- Analisa o perfil do candidato e os requisitos da vaga
- Gera texto explicativo sobre o motivo da recomendaÃ§Ã£o positiva ou negativa
- Adapta o tom e conteÃºdo com base na probabilidade da prediÃ§Ã£o
- Inclui detalhes relevantes como experiÃªncia e formaÃ§Ã£o do candidato

### Como Funciona
O sistema utiliza:
1. **TextBlob** para processamento de linguagem natural
2. **Templates personalizados** para diferentes cenÃ¡rios de recomendaÃ§Ã£o
3. **LÃ³gica contextual** para selecionar os detalhes mais relevantes a destacar

Para mais detalhes sobre esta funcionalidade, consulte a documentaÃ§Ã£o em [docs/llm_comments.md](docs/llm_comments.md)

## ğŸ”® PrÃ³ximos Passos

1. **ExperimentaÃ§Ã£o com MLflow**: 
   - âœ… OtimizaÃ§Ã£o de hiperparÃ¢metros - implementado com RandomizedSearchCV
   - âœ… Teste de diferentes algoritmos - implementado com comparaÃ§Ã£o de modelos
   - âœ… ComparaÃ§Ã£o de mÃ©tricas de desempenho - tracking com MLflow

2. **ImplementaÃ§Ã£o de Clustering**:
   - SegmentaÃ§Ã£o de candidatos por perfil
   - IdentificaÃ§Ã£o de grupos de vagas similares
   - IntegraÃ§Ã£o do clustering ao scoring model

3. **Melhorias no Modelo**:
   - Feature engineering avanÃ§ado
   - ImplementaÃ§Ã£o de tÃ©cnicas de deep learning
   - ValidaÃ§Ã£o cruzada para maior robustez

4. **ProdutivizaÃ§Ã£o**:
   - âœ… API REST para servir o modelo - implementada com FastAPI
   - âœ… ImplantaÃ§Ã£o no Render - configurada com Docker
   - âœ… ComentÃ¡rios LLM para explicabilidade - implementado com TextBlob
   - Monitoramento contÃ­nuo de performance
   - Pipeline de retreinamento automÃ¡tico

## ğŸ¤ ContribuiÃ§Ã£o

1. FaÃ§a um fork do projeto
2. Crie uma branch para sua feature (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudanÃ§as (`git commit -m 'Add some AmazingFeature'`)
4. Push para a branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a Apache 2.0. Veja o arquivo [`LICENSE`](LICENSE) para mais detalhes.

## ğŸ§ª Testes UnitÃ¡rios

O projeto conta com testes unitÃ¡rios para garantir a qualidade e o funcionamento correto dos componentes da pipeline. Para executar os testes:

```bash
# Execute o script de testes
./scripts/run_tests.sh
```

Para mais detalhes sobre os testes implementados, consulte o [README dos testes](tests/README.md).

## ğŸ“ Contato

FIAP Tech Challenge 05 - [@rivolela](https://github.com/rivolela/fiap_tech_challenge_05)

---
**Nota**: Certifique-se de que os arquivos JSON estejam disponÃ­veis na pasta `data/` antes de executar o notebook de anÃ¡lise.


